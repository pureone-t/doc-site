---
title: 機械学習 入門#1
date: 2018-12-07T23:00:00+09:00
showDate: true
tags: ["DataScience"]
---

# 1基礎
## 機械学習の位置付け
人工知能 > 機械学習 > ディープラーニング

```
目 -> 画像 -> 数値 -> 機械学習
耳 -> 音声(時系列を含む) -> 数値 -> 機械学習
口 -> 自然言語 -> 数値(word2vec) -> 機械学習

入力𝑥 -> 機械学習 -> 出力𝑦
=> 入力𝑥 と 出力𝑦 の関係性(規則性)を見つける

機械学習 {
  ディープラーニング、
  単回帰分析、
  SVM(Support Vector Machine)
}
```

## 機械学習に必要な数学
- 微分積分
- 線形代数
- 確率・統計
ディープラーニングは、微分と線形代数が重要！

## 機械学習の３大トピック
- 教師あり学習（入力$x$と出力$y$）
  - 回帰：数値を予測（家賃 50000円） = 広さ(𝑥) -> 家賃($y$)
  - 分類：カテゴリを分類（赤ワイン or 白ワイン）= アルコール($x$) -> 種類($y$)
- 教師なし学習（入力$x$）
  - クラスタリング（グルーピング）
  - 次元削減（入力変数を減らす）
- 強化学習（データがない or ほとんどない）
  - 掃除ロボット

## 機械学習と内挿・外挿
- 内挿：持っている入力データ内で予測すること（1~4）
- 外挿：持っている入力データ外で予測すること

機械学習は、内挿を保証するため、外挿は取り扱わない

### 株価について
価格と時間のデータ（時系列データ）は、縦軸の価格の範囲が内挿になるため、
明日以降のデータを予測することは、機械学習の範囲である。

## 微分
- 微分は「何」が求まるのか？
  - 接線の傾き（$y = ax^2$ の U字の放物線において 、接線 $y = ax + b$ の $a$）

- 微分は「何」に使えるのか？
  - 傾き「0」を利用することで、ある関数が（例：誤差）最小（もしくは最大）となる点が求まる
  - `誤差 = 実際 - 予測`

## 微分（導関数）を求める
{$x1$,$f(x1)$}、{$x2$,$f(x2)$} の2点を通る接線の傾き$a$

$a = \frac{yの増加量}{xの増加量}$

$a = \frac{f(x_2)-f(x_1)}{x_2-x_1}$

### 極限
{$x$, $f(x)$} の1点を通る接戦の傾き$a$  
増加量は $h$ とする

$a = \displaystyle \lim_{ h \to 0 } \frac{f(x+h)-f(x)}{h}$

## 微分の公式
### 記号
$()' = \frac{ dy }{ dx }()$

### 暗記
$(1)' = 0$  
$(x)' = 1$  
$(x^2)' = 2x$

### 例題
$(x^2)' = 2x$ を解いていく  

<公式>  
$f'(x) = \displaystyle \lim_{ h \to 0 } \frac{f(x+h)-f(x)}{h}$  

<代入>  
$f(x) = x^2$  
$f(x+h) = (x+h)^2 = x^2 + 2xh + h^2$  

<計算>  
$(x^2) = \displaystyle \lim_{ h \to 0 } \frac{(x^2 + 2xh + h^2)-x^2}{h}$  

$(x^2) = \displaystyle \lim_{ h \to 0 } \frac{2xh + h^2}{h}$  

$(x^2) = \displaystyle \lim_{ h \to 0 } (2x + h)$  

$(x^2) = (2x + 0)$  

$(x^2) = 2x$  

### 例題1
$(3x^2)' = 3 \times (x^2)' = 6x$

### 例題2
$(4x+3)' = (4x)' + (3)' = 4 \times (x)' + 3 \times (1)' = 4 \times 0 = 4$

## 偏微分
多変数の微分
```
𝑦 (家賃)
├─x_1 (距離)
├─...
└─x_m (治安)
```

### 公式
$\partial$ =ラウンド、ディー

$\frac{ \partial }{ \partial a }$

$a$ で偏微分する = $a$ 以外を定数だと仮定して微分する

### 例題1
<代入>  
$(a^2) = (x^2)' = 2a$

<計算>
$\frac{ \partial }{ \partial a }(3a^2) = 3 \times \frac{ \partial }{ \partial a }(a^2) = 3 \times (a^2) = 3 \times 2a = 6a$

### 例題2
$\frac{ \partial }{ \partial a }(4x_1 + 3x_2)$

$\frac{ \partial }{ \partial a }(4x_1) + \frac{ \partial }{ \partial a }(3x_2)$

$4 \times \frac{ \partial }{ \partial a }(x_1) + 3x_2 \times \frac{ \partial }{ \partial a }(1)$

$4 \times 1 + 3x_2 \times 0 = 4$

### 例題3
$\frac{ \partial }{ \partial a }(c_0 - 2c_1a + c_2a^2) = -2c_1 + 2c_2a$

# 単回帰分析
## 問題設定
- 例 家賃の予測
  - 出力変数：$y$ (予測したいもの)
  - 入力変数：$x$ (予測するために使うファクター、広さ、距離)
  - 何か $x$ ひとつから $y$ を予測する
  - 複数の $x$ から $y$ を予測したい場合は、重回帰分析を使う

## フェーズ
- 学習
  - データ（広さが〇〇のとき、家賃が■■）を大量にモデルとして用意する
- 推論
  - 学習済みモデルは、過去のデータから予測する(=予測値を算出する)
- どうやって学習させるか(=部屋の広さと家賃の関係性)

## STEP1 モデルを決める
- グラフや表で整理する
- 直線であれば $\hat{ y } = ax + b$ で決めれる ($\hat{ y }$ は予測値)
- データに基づいてパラメータ $a, b$ を決定する

センタリング(データの中心化)を活用すると、切片を0とすることができる。  
- $\bar{ y }$：$y$ の平均  
- $\bar{ x }$：$x$ の平均

## STEP2 評価関数(損失関数)を決める
= 誤差の算出

- 実：$y$
- 予：$\hat{ y }$
- 実 - 予 = 離れている差分
- $(実 - 予)^2$ することでプラスの値かつ、グラフ上はU字の曲線を描くため、微分の計算が可能となる
- 実と予の二重の誤差を算出する(二重誤差)

$\mathcal{L}(評価関数) = (y_1 - \hat{y_1})^2 + (y_2 - \hat{y_2})^2 ...+ (y_n - \hat{y_N})^2$  
($N$ はサンプル数)

$\displaystyle \sum_{n=1}^{N} (y_n - \hat{y_N})^2$

## STEP3 評価関数(損失関数)を最小化する
= 誤差の小さくする (=傾き0)

$\frac{ \partial }{ \partial a }(\mathcal{L}) = 0$

### STEP3-1 式変形
$\hat{y_{N}} = ax_n$

$\mathcal{L} = \displaystyle \sum_{n=1}^{N} (y_n - ax_n)^2$

$= \displaystyle \sum_{n=1}^{N} (y_n^2 - 2y_nax_n + a^2x_n^2)$

$= \displaystyle \sum_{n=1}^{N} (y_n^2 - 2y_nx_na + x_n^2a^2)$

$\displaystyle \sum_{n=1}^{N}y_n^2$ = $c_0$

$\displaystyle \sum_{n=1}^{N}x_ny_n$ = $c_1$

$\displaystyle \sum_{n=1}^{N}x_n^2$ = $c_2$

$= c_0 - 2c_1a + c_2a^2$

### STEP3-2 最適なパラメータを求める
$\mathcal{L} = c_0 - 2c_1a + c_2a^2$

$\frac{ \partial }{ \partial a }(\mathcal{L}s) = 0$

$\frac{ \partial }{ \partial a }(c_0 - 2c_1a + c_2a^2) = 0$

$-2c_1 \times \frac{\partial}{\partial a}(a) + c_2 \times \frac{\partial}{\partial a}(a^2) = 0$

$-2c_1 \times + 2c_2a = 0$

$2c_2a = 2c_1$

$a = \frac{c_1}{c_2}$

$$
a = \frac{\displaystyle{\sum_{n=1}^{N}}x_{n}y_{n}}
{\displaystyle{\sum_{n=1}^{N}}x_{n}^{2}}
$$
